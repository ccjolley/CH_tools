{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling demo (Arabic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import pandas as pd\n",
    "import lda\n",
    "from glob import glob\n",
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "The data we're using for this demo comes from the لنتفق الآن (\"Let's Agree Now!\") Facebook page. Most posts are in Arabic, and they will address a variety of different topics. Some posts come from the account owner, while the majority will be contributed by visitors to their page. The first five entries are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Franc Slame</td>\n",
       "      <td>انتو تفرحو بناقلت نفط ونحن نفرح لتحري أرض الوطن ياانجاس</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Salma Abu</td>\n",
       "      <td>وأخيرا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>الحفره الحفره</td>\n",
       "      <td>سبب الخراب والدمار فبراير اليوم الاسود 2011/2/17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>غدا اجمل</td>\n",
       "      <td>يا خوي اتق الله كيف تحلف بالله علي شي في علم الغيب .. الله وحده العالم بشن حيصير في البلاد .. ومن اللي حيرضى انه بلادنا تنباع ؟؟؟؟</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abdo Altwel</td>\n",
       "      <td>مفروض كل واحد فيهم ايروح على رجليه لن  يوصل حوشه تو بعدين يتعلمو التشحيط</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Author  \\\n",
       "0    Franc Slame   \n",
       "1      Salma Abu   \n",
       "2  الحفره الحفره   \n",
       "3       غدا اجمل   \n",
       "4    Abdo Altwel   \n",
       "\n",
       "                                                                                                                             Contents  \n",
       "0                                                                             انتو تفرحو بناقلت نفط ونحن نفرح لتحري أرض الوطن ياانجاس  \n",
       "1                                                                                                                              وأخيرا  \n",
       "2                                                                                    سبب الخراب والدمار فبراير اليوم الاسود 2011/2/17  \n",
       "3  يا خوي اتق الله كيف تحلف بالله علي شي في علم الغيب .. الله وحده العالم بشن حيصير في البلاد .. ومن اللي حيرضى انه بلادنا تنباع ؟؟؟؟  \n",
       "4                                                            مفروض كل واحد فيهم ايروح على رجليه لن  يوصل حوشه تو بعدين يتعلمو التشحيط  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xls = pd.concat([pd.read_excel(x) for x in glob('Lets Agree Now*.xls')])\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "xls[['Author','Contents']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "In the field of Natural Language Processing (NLP), certain words are known as *stop words*. These are very common words -- English examples could include \"the\", \"of\", \"is\", \"and\", \"or\", etc. -- that often don't tell us very much about the subject of the sentence that contains them. We'll be using a set of common Arabic stop words that are commonly used by NLP researchers. Here are the first 20, to give a sense of what kind of words we're talking about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['فى', 'في', 'كل', 'لم', 'لن', 'له', 'من', 'هو', 'هي', 'قوة', 'كما', 'لها', 'منذ', 'وقد', 'ولا', 'نفسه', 'لقاء', 'مقابل', 'هناك', 'وقال']\n"
     ]
    }
   ],
   "source": [
    "print(get_stop_words('arabic')[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for topic modeling\n",
    "Topic modeling algorithms often struggle to identify the topic of very short pieces of text. We'll get around this by only paying attention to posts with more than 250 characters -- there are 2,342 such posts. We're also going to simplify things by removing numbers and URLs, since they're not likely to tell us much about topics.\n",
    "\n",
    "Those long posts are then passed to an algorithm called a *vectorizer* that turns the set of posts into a matrix of numbers, because computers generally prefer to work with numbers. This is done by identifying the *vocabulary* of all words that appear at least once (45,086 of them) and counting the number of times that each word appears in each post. This means we have a 2342x45086 array of numbers (known as a *document-term matrix* or DTM), for 2,342 documents and 45,086 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xls['Filtered'] = xls['Contents'].replace(r'\\d+','',regex=True)\n",
    "xls['Filtered'] = xls['Filtered'].replace(r'https?://[\\w./]+','',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2342, 45086)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getlen(x):\n",
    "    try:\n",
    "        return(len(x))\n",
    "    except:\n",
    "        return(0)\n",
    "xls['strlen'] = xls['Filtered'].apply(getlen)\n",
    "long_posts = xls[xls.strlen > 250].reset_index(drop=True)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(stop_words=get_stop_words('arabic')) \n",
    "vect.fit(long_posts.Filtered)\n",
    "long_dtm = vect.transform(long_posts.Filtered)\n",
    "vocab = vect.get_feature_names()\n",
    "long_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the topic model\n",
    "The algorithm we're using is called LDA (Latent Dirichlet Allocation). The basic idea is that we give it a set of documents (or more precisely a DTM), and ask it to identify a specific number of topics (20 in this case). Each document is given a *probability* of belonging to each topic. For each topic, every word has a *weight* -- the words with the highest weights are the ones that are most important to the topic.\n",
    "\n",
    "Part of the model-fitting algorithm requires a random-number generator -- this means that the results will be slightly different each time. We're going to fit the model twice, because we suspect that topics that are consistent between the two versions of the fitted model will be more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lda.lda.LDA at 0x270a13526d8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myLDA1 = lda.LDA(n_topics=20, n_iter=1500, random_state=1)\n",
    "myLDA1.fit(long_dtm)\n",
    "myLDA2 = lda.LDA(n_topics=20, n_iter=1500, random_state=2)\n",
    "myLDA2.fit(long_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tw1 = myLDA1.topic_word_\n",
    "tw2 = myLDA2.topic_word_\n",
    "ldamat1 = myLDA1.transform(long_dtm)\n",
    "ldamat2 = myLDA2.transform(long_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing topics\n",
    "We can quickly get a sense of what some of the major topics on the \"Let's Agree Now!\" Facebook page are by looking at the 8 most heavily-weighted terms in each topic. Here are the results for the first version of the fitted model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: الله لله الملك مافي مابعده ليبيا type timeline\n",
      "Topic 1: أن أو لو ليس جدا al mostakbal الحياة\n",
      "Topic 2: ليبيا أن إلى المتحدة الليبي حكومة كوبلر السياسي\n",
      "Topic 3: اللي مش علي والله الي بس البلاد يا\n",
      "Topic 4: أن إلى وكالة التضامن أنباء أي الليبية أنه\n",
      "Topic 5: الآن لنتفق لنتفق_الآن type photos timeline ليبيا أو\n",
      "Topic 6: visa the ﻣﻦ ﺑﺎﻟﻠﻪ on ﺍﻟﻠﻪ ﺇﻻ arrival\n",
      "Topic 7: اللهم يا الله يارب أن لي خير ربي\n",
      "Topic 8: وزارة أن الحكومة حكومة وزير الخارجية تشكيلة بأن\n",
      "Topic 9: الوطني المؤتمر الوفاق حكومة إن السياسي الليبي مجلس\n",
      "Topic 10: ليبيا الوطن الشعب الله ولكن الليبيين الليبي البلاد\n",
      "Topic 11: إلى أن داعش ﺍﻛﺒﺮ ﺍﻟﻠﻪ الدولة ليبيا الليبي\n",
      "Topic 12: النواب مجلس أن المجلس الوفاق الثقة الاتفاق جلسة\n",
      "Topic 13: الله ال وا محمد ين يا وسلم الل\n",
      "Topic 14: ليبيا علي الي الجيش الغرب حفتر طرابلس الناس\n",
      "Topic 15: ليبيا لنتفق المركزي لنتفق_الآن النفط الآن مصرف المواطن\n",
      "Topic 16: المجلس الرئاسي الوفاق الوطني لحكومة رئيس السراج طرابلس\n",
      "Topic 17: الآن لنتفق لنتفق_الآن type طرابلس مدينة تشجيع photos\n",
      "Topic 18: الله الوكيل ونعم حسبي حسبنا عل سيدنا شاء\n",
      "Topic 19: اللي بنغازي سيتم الوفاق تعليقات لانه بـ التوافق\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 8\n",
    "\n",
    "def top_words(topic_dist,n_top_words=10):\n",
    "    return(np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1])\n",
    "\n",
    "for i, topic_dist in enumerate(tw1):\n",
    "    topic_words = top_words(topic_dist,8)\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of how much we can trust the topics, we can look at the same output for the other version of the model and see which topics look similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: لنتفق الآن لنتفق_الآن type photos ليبيا timeline أو\n",
      "Topic 1: النواب مجلس أن جلسة الثقة الوفاق المجلس عضو\n",
      "Topic 2: الوطني الوفاق حكومة إن السيد الليبي رئيس مجلس\n",
      "Topic 3: وزارة محمد عل وزيرا سيدنا زليتن آل وعلى\n",
      "Topic 4: الله الوكيل ونعم حسبي حسبنا شاء فيكم سبحان\n",
      "Topic 5: المجلس الرئاسي الوفاق لحكومة الوطني رئيس السراج ليبيا\n",
      "Topic 6: اللي مش علي البلاد ليبيا بس توا فينا\n",
      "Topic 7: ليبيا المتحدة أن الليبي المؤتمر الآن الأمم لنتفق\n",
      "Topic 8: لنتفق الآن لنتفق_الآن type مدينة طرابلس إلى تشجيع\n",
      "Topic 9: الله لله الملك مافي مابعده timeline الآن photos\n",
      "Topic 10: أن إلى حكومة الوفاق مجلس السياسي الوطني المجلس\n",
      "Topic 11: ﺍﻟﻠﻪ ﺍﻛﺒﺮ ﻣﻦ ﺑﺎﻟﻠﻪ داعش ﺇﻻ قوات ﻭﻻ\n",
      "Topic 12: ال الله وا ين الل ون وسلم صل\n",
      "Topic 13: أن ليبيا داعش إلى إن الليبيين الشعب الاتحاد\n",
      "Topic 14: ليبيا المركزي مصرف دولار دينار الليبية السيولة المواطن\n",
      "Topic 15: اللهم الله يا أن يارب العظيم لي خير\n",
      "Topic 16: طرابلس النفط الوطنية المنطقة بنغازي أي المدني أن\n",
      "Topic 17: ليبيا الشعب الجيش الليبي الي الغرب العالم النظام\n",
      "Topic 18: علي بنغازي يا الي الله والله الناس لو\n",
      "Topic 19: الوطن أو نحن ولكن يجب الله تكون الليبيين\n"
     ]
    }
   ],
   "source": [
    "for i, topic_dist in enumerate(tw2):\n",
    "    topic_words = top_words(topic_dist,8)\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding matching topics\n",
    "We know that only some topics will overlap between the two sets, and that the similar topics may be in a different order. We can easily measure the similarity between two topics by listing the top 100 words in each topic, and counting how many of those top words are in common between the two lists. If two topics have more than 50% of their top 100 words in common, we'll consider them to be a match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 9), (2, 7), (2, 10), (3, 6), (5, 0), (7, 15), (9, 2), (12, 1), (13, 12), (15, 14), (16, 5), (17, 8)]\n"
     ]
    }
   ],
   "source": [
    "def overlap(td1,td2,n=10):\n",
    "    w1 = top_words(td1,n)\n",
    "    w2 = top_words(td2,n)\n",
    "    return(len(set(w1) & set(w2))/n)\n",
    "\n",
    "match_pairs = []\n",
    "n = tw1.shape[0]\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        o = overlap(tw1[i,:],tw2[j,:],100)\n",
    "        if o > 0.5:\n",
    "            match_pairs.append((i,j))\n",
    "            \n",
    "print(match_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of our two sets of 20 topics, we have 12 matches. The numbers above mean that, for example, topic \\#0 from the first model matches with \\#9 from the second model, while topic \\#2 from the first model could match with \\#7 or \\#10 from the second.\n",
    "## Example posts\n",
    "Each post has a probability -- a number between 0 and 1 -- measuring how likely it is to fit in with any topic. To better understand what is in a topic, we can find a post with a very high probability for that topic. The example below is for topic \\#9 in the first model, which (as far as I can tell from Google Translate) contains terms related to \"national\", \"government\", \"conference\", and \"accord.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "لنتفق الآن\n",
      "\n",
      "https://www.facebook.com/Lets.Agree.LY/photos/a.1500669000237089.1073741828.1500579190246070/1504781203159202/?type=3\n",
      "\n",
      "لنتفق الآن\n",
      "\n",
      "موجز رقم 2\n",
      "\n",
      "موجز لأبرز ما تداولته وسائل الإعلام اليوم الأثنين 21.12.2015 .\n",
      "\n",
      "ومواصلة في رصد اخر اخبار ملف الحوار الليبي - اللبيبيفقد ذكر السيد \" سعيد الختالي \"عضو المؤتمر الوطني العام ن المؤتمر ومجلس النواب شكلا يوم السبت الماضي لجنة مشتركة للحوار بينهما، مكونة من 34 عضوا نصفهم من المؤتمر الوطني ونصفهم الآخر من مجلس النواب، وبين الختالي وبيّن الختالي أن للجنة المشتركة مهام عدة أولها تشكيل حكومة وفاق وطني. وعلى صعيدا اخر اصدر مجلس البحوث والدراسات الشرعية بدارالإفتاء بيانا عبر فيه عن دعمه للحوار الليبي الليبي المتمثل في إعلان المبادئ بتونس ، ودعا المجلس في بيانه كل من المؤتمر الوطني ومجلس النواب إلى الإسراع بتشكيل حكومة وفاق وطني تنهي الانقسام في السلطة ، مطالبا المجتمع الدولي بالتريث ودعم الوفاق الليبي الليبي. هذا وتجدر الإشارة ان السيد \" عقيلة صالح \" رئيس مجلس النواب قد طالب المجتمع الدولي بدعم الحوار الليبي الليبي الذي استهل بلقاء بينه وبين رئيس المؤتمر الوطني العام نوري أبوسهمين.\n",
      "\n",
      "موجز رقم 2 لأهم ما تناولته وسائل الإعلام البرحة الأثنين 21.12.2015\n"
     ]
    }
   ],
   "source": [
    "# Show key words and example post for a given match pair\n",
    "def topic_example(i,n=10000):\n",
    "    print(long_posts.Contents[np.argmax(ldamat1[:,i])][:n])\n",
    "\n",
    "i = 9\n",
    "topic_example(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
